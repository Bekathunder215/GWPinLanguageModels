scenarios:
  one:
    # I/O
    OUT_DIR: "out"
    DATA_DIR: "data"
    EVAL_INTERVAL: 200
    EVAL_ITERS: 50
    LOG_INTERVAL: 50
    SAVE_CHECKPOINT: True
    EMISSIONS_DIR: "one"

    # Model (main tunables)
    N_LAYER: 4
    N_HEAD: 4
    N_EMBD: 128
    DROPOUT: 0.1
    BIAS: True

    # Training (main parameters you can also experiment with)
    SEED: 1
    DEVICE: "cpu" # If you can, try also seeing consumption when using gpu (change this to 'cuda' if torch.cuda.is_available() else 'cpu')
    DTYPE: "float32"
    BATCH_SIZE: 32 # Number of sequences processed in parallel.
    BLOCK_SIZE: 256 # Maximum context length for predictions (e.g. 128 or 256). The longer the block size, the more memory and compute it requires, but it can also lead to better performance.
    MAX_ITERS: 500 # Total number of training iterations. The more iterations, the better the model can perform, but it also takes more time and energy to train.
    LEARNING_RATE: 3e-4 # the standard starting learning rate, often good enough for a first try
    WEIGHT_DECAY: 0.1 # L2 Regularization
    GRAD_CLIP: 1.0 # To prevent exploding gradients
  two:
    # I/O
    OUT_DIR: "out"
    DATA_DIR: "data"
    EVAL_INTERVAL: 200
    EVAL_ITERS: 50
    LOG_INTERVAL: 50
    SAVE_CHECKPOINT: True
    EMISSIONS_DIR: "two"

    # Model (main tunables)
    N_LAYER: 8
    N_HEAD: 8
    N_EMBD: 128
    DROPOUT: 0.2
    BIAS: True

    # Training (main parameters you can also experiment with)
    SEED: 1
    DEVICE: "cpu" # If you can, try also seeing consumption when using gpu (change this to 'cuda' if torch.cuda.is_available() else 'cpu')
    DTYPE: "float32"
    BATCH_SIZE: 64 # Number of sequences processed in parallel.
    BLOCK_SIZE: 256 # Maximum context length for predictions (e.g. 128 or 256). The longer the block size, the more memory and compute it requires, but it can also lead to better performance.
    MAX_ITERS: 500 # Total number of training iterations. The more iterations, the better the model can perform, but it also takes more time and energy to train.
    LEARNING_RATE: 2e-4 # the standard starting learning rate, often good enough for a first try
    WEIGHT_DECAY: 0.1 # L2 Regularization
    GRAD_CLIP: 1.0 # To prevent exploding gradients
